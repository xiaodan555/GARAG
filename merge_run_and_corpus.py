import json
import os
import csv
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# BEIR æ ¹ç›®å½•
BEIR_ROOT = "data/beir"
# ===========================================

def load_qrels(path):
    """åŠ è½½æ ‡å‡†ç­”æ¡ˆæ˜ å°„ (QID -> List[DocID])"""
    qrels = {}
    if not os.path.exists(path):
        return qrels
    print(f"   - æ­£åœ¨åŠ è½½ Qrels: {path}")
    with open(path, 'r') as f:
        reader = csv.reader(f, delimiter='\t')
        next(reader, None) # è·³è¿‡è¡¨å¤´
        for row in reader:
            qid, doc_id = row[0], row[1]
            if qid not in qrels: qrels[qid] = []
            qrels[qid].append(doc_id)
    return qrels

def process_dataset(dataset_name):
    # è·¯å¾„é…ç½®
    base_path = os.path.join(BEIR_ROOT, dataset_name)
    run_file = os.path.join(base_path, f"run_contriever_{dataset_name}_top100.json")
    output_file = os.path.join(base_path, f"{dataset_name}_garag_ready.json")

    print(f"\nğŸš€ å¼€å§‹æ‹¼æ¥æ•°æ®é›†: {dataset_name}")
    print(f"ğŸ“‚ è¯»å– Run File: {run_file}")
    
    if not os.path.exists(run_file):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° Run Fileï¼è¯·ç¡®è®¤ä½ æ˜¯å¦è¿è¡Œäº†æ£€ç´¢è„šæœ¬ ({run_file})ã€‚")
        return

    # 1. åŠ è½½ Run File (æ£€ç´¢ç»“æœ)
    with open(run_file, 'r') as f:
        run_data = json.load(f)
    
    target_qids = list(run_data.keys())
    print(f"   - åŒ…å« {len(target_qids)} ä¸ªé—®é¢˜")

    # 2. æ”¶é›†æ‰€æœ‰éœ€è¦æå–çš„æ–‡æ¡£ ID (åŒ…æ‹¬ Top-100 å’Œ æ ‡å‡†ç­”æ¡ˆæ–‡æ¡£)
    # æˆ‘ä»¬åªéœ€è¦åŠ è½½è¿™äº›æ–‡æ¡£çš„å†…å®¹ï¼Œä¸éœ€è¦åŠ è½½æ•´ä¸ª 200ä¸‡ Corpusï¼Œçœå†…å­˜
    needed_doc_ids = set()
    for qid, docs in run_data.items():
        for doc_id in docs.keys():
            needed_doc_ids.add(doc_id)
            
    # 3. åŠ è½½ Qrels (ä¸ºäº†æ ‡è®° has_answer)
    # ğŸ”§ ä¿®æ”¹ï¼šé’ˆå¯¹ MS MARCO å¼ºåˆ¶ä½¿ç”¨ devï¼Œé˜²æ­¢è¯»åˆ°ç©ºçš„ test æ–‡ä»¶
    if dataset_name == "msmarco":
        qrels_path = os.path.join(base_path, 'qrels', 'dev.tsv')
        print("ğŸ”§ æ£€æµ‹åˆ° MS MARCOï¼Œå¼ºåˆ¶åŠ è½½ Qrels: dev.tsv")
    else:
        # å…¶ä»–æ•°æ®é›†ä¼˜å…ˆæ‰¾ test
        qrels_path = os.path.join(base_path, 'qrels', 'test.tsv')
        if not os.path.exists(qrels_path):
            qrels_path = os.path.join(base_path, 'qrels', 'dev.tsv')
            
    qrels = load_qrels(qrels_path)
    
    # æŠŠæ ‡å‡†ç­”æ¡ˆçš„ Doc ID ä¹ŸåŠ è¿›å»ï¼Œé˜²æ­¢æ£€ç´¢æ²¡å¬å›å¯¼è‡´æŠ¥é”™
    for qid in target_qids:
        if qid in qrels:
            for gold_doc_id in qrels[qid]:
                needed_doc_ids.add(gold_doc_id)

    print(f"   - éœ€è¦æå–çš„æ–‡æ¡£æ€»æ•°: {len(needed_doc_ids)}")

    # 4. æ‰«æ Corpus (æå–å†…å®¹)
    doc_lookup = {}
    corpus_path = os.path.join(base_path, 'corpus.jsonl')
    print(f"ğŸ“‚ æ‰«æ Corpus: {corpus_path} (è¯·ç¨å€™)...")
    
    if not os.path.exists(corpus_path):
         print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° Corpus File ({corpus_path})")
         return

    with open(corpus_path, 'r', encoding='utf-8') as f:
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼Œå› ä¸º MSMARCO å¾ˆå¤§
        for line in tqdm(f, desc=f"Reading Corpus ({dataset_name})"):
            # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœè¿™ä¸€è¡ŒåŒ…å«æˆ‘ä»¬éœ€è¦çš„IDï¼Œå†è§£æ JSON (æå¤§æå‡é€Ÿåº¦)
            # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…ä¼˜åŒ–ï¼Œé˜²æ­¢ json.loads æ¯ä¸€è¡Œ
            # è™½ç„¶æœ‰è¯¯åˆ¤å¯èƒ½ï¼Œä½†åœ¨ doc_id è¾ƒé•¿æ—¶å¾ˆæœ‰æ•ˆã€‚
            # ä¸ºç¨³å¦¥èµ·è§ï¼Œæˆ‘ä»¬è¿˜æ˜¯è€è€å®å®è§£æï¼Œä½†åªå­˜éœ€è¦çš„
            # ä¸ºäº†æ€§èƒ½ï¼Œå¯ä»¥å°è¯•ç®€å•å­—ç¬¦ä¸² checkï¼Œä½†è¿™é‡Œä¸ºäº†ä¿é™©ç›´æ¥ json.loads
            # å¦‚æœè§‰å¾—æ…¢ï¼Œå¯ä»¥å…ˆ check string in line
            item = json.loads(line)
            if item['_id'] in needed_doc_ids:
                doc_lookup[item['_id']] = {
                    "title": item.get("title", ""),
                    "text": item.get("text", "")
                }
    
    # 5. åŠ è½½ Queries (è·å–é—®é¢˜æ–‡æœ¬)
    print("ğŸ“‚ åŠ è½½ Queries...")
    query_lookup = {}
    queries_path = os.path.join(base_path, 'queries.jsonl')
    if not os.path.exists(queries_path):
         print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° Queries File ({queries_path})")
         return

    with open(queries_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            if item['_id'] in target_qids:
                query_lookup[item['_id']] = item['text']

    # 6. ç»„è£…æœ€ç»ˆæ•°æ®
    print("ğŸ”¨ æ­£åœ¨ç»„è£…æœ€ç»ˆ JSON...")
    final_data = []
    
    for qid in target_qids:
        if qid not in query_lookup:
            continue
            
        question_text = query_lookup[qid]
        gold_doc_ids = qrels.get(qid, [])
        
        # æ„å»º ctxs åˆ—è¡¨
        ctxs = []
        top_docs = run_data[qid] # è¿™æ˜¯ä¸€ä¸ª dict: {doc_id: score}
        
        # æŒ‰åˆ†æ•°æ’åºç¡®ä¿é¡ºåºæ­£ç¡®
        sorted_docs = sorted(top_docs.items(), key=lambda x: x[1], reverse=True)
        
        for doc_id, score in sorted_docs:
            if doc_id in doc_lookup:
                doc_content = doc_lookup[doc_id]
                is_gold = doc_id in gold_doc_ids
                
                ctxs.append({
                    "id": doc_id,
                    "title": doc_content['title'],
                    "text": doc_content['text'],
                    "score": score,
                    "has_answer": is_gold # è¿™ä¸€é¡¹å¯¹ GARAG å¾ˆé‡è¦
                })
        
        # âš ï¸ BEIR æ•°æ®é›†é€šå¸¸åªæœ‰ Doc ID ä½œä¸ºç­”æ¡ˆï¼Œæ²¡æœ‰çŸ­è¯­æ–‡æœ¬ç­”æ¡ˆ
        # GARAG è¿™é‡Œçš„ answers å­—æ®µå¦‚æœä¸å¡«å¯èƒ½ä¼šæŠ¥é”™ï¼Œæˆ–è€…è¯„ä¼°ä¸º 0
        # æˆ‘ä»¬è¿™é‡Œå¡«å…¥ "Unknown" å ä½ã€‚
        # (GARAG çš„æ”»å‡»é€šå¸¸å…³æ³¨æ£€ç´¢æ’åºï¼Œåªè¦ has_answer æ ‡è®°å¯¹å°±è¡Œ)
        final_data.append({
            "question": question_text,
            "answers": ["Unknown"], 
            "ctxs": ctxs
        })

    # 7. ä¿å­˜
    print(f"ğŸ’¾ ä¿å­˜ç»“æœè‡³: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, indent=4)
        
    print(f"âœ… [{dataset_name}] å¤„ç†å®Œæˆï¼å·²ç”Ÿæˆ {len(final_data)} æ¡å®Œæ•´æ•°æ®ã€‚")


def main():
    datasets = ["nq", "hotpotqa", "msmarco"]
    for ds in datasets:
        process_dataset(ds)

if __name__ == "__main__":
    main()

# import random
# import torch
# import numpy as np
# from src.option import Options
# from src.util import init_logger, timestr
# from src.task import ReaderDataset, evaluate
# from src.attacker import build_attack
# from textattack.augmentation import Augmenter
# from textattack.attack_args import AttackArgs

# import tqdm
# import os
# import json
# import logging

# from textattack.metrics.quality_metrics import Perplexity, USEMetric
# from textattack.shared import AttackedText, utils

# logger = logging.getLogger(__name__)

# os.environ["TOKENIZERS_PARALLELISM"] = "false"

# # def main():
# #     option = Options("attack")
# #     opt, message = option.parse(timestr())
# #     logger = init_logger(opt)
# #     logger.info(message)
# #     logger.info("The name of experiment is {}".format(opt.name))
# #     logger.info("Attack type is {}".format(opt.method))

# #     dataset = ReaderDataset(opt)
# #     attack, dataset = build_attack(opt, dataset)

# #     if opt.is_black:
# #         result = attack.augment_dataset(dataset)

# #     elif opt.is_genetic:
# #         result = attack.attack_dataset(dataset)

# #     elif opt.is_hotflip:
# #         result = attack.attack_dataset(dataset)
# #     else:
# #         result = attack.attack_dataset(dataset)
# #     logger.info("Attack finished")
# #     evaluate(result)
# #     if opt.is_save:
# #         # data_dir = os.path.join(os.path.split(opt.data_dir)[0], "noise", "g_p_{}_seq_{}".format(opt.perturbation_level, opt.transformations_per_example))
# #         # os.makedirs(data_dir, exist_ok=True)
# #         with open(os.path.join(opt.output_dir, "{}.json".format(opt.method)), 'w') as f: json.dump(result,f)
    

# # if __name__=="__main__":
# #     main()

# # è¾…åŠ©å‡½æ•°ï¼šæŠŠ Tensor å˜æˆæ™®é€šæ•°å­—ï¼Œé˜²æ­¢ JSON æŠ¥é”™
# def make_serializable(obj):
#     if isinstance(obj, torch.Tensor):
#         return obj.item() if obj.numel() == 1 else obj.tolist()
#     elif isinstance(obj, np.ndarray):
#         return obj.tolist()
#     elif isinstance(obj, (np.float32, np.float64)):
#         return float(obj)
#     elif isinstance(obj, (np.int32, np.int64)):
#         return int(obj)
#     elif isinstance(obj, list):
#         return [make_serializable(i) for i in obj]
#     elif isinstance(obj, dict):
#         return {k: make_serializable(v) for k, v in obj.items()}
#     return obj

# def main():
#     # 1. åˆå§‹åŒ–é…ç½®
#     t = timestr()
#     op = Options("attack")
#     opt, message = op.parse(t)
    
#     # 2. åˆå§‹åŒ–æ—¥å¿— (æ¢å¤åŸä½œè€…é€»è¾‘ï¼Œè¿™æ ·æ—¥å¿—ä¼šå­˜åˆ° output æ–‡ä»¶å¤¹)
#     global logger
#     logger = init_logger(opt)
#     logger.info(message)
#     logger.info(f"Experiment Name: {opt.name}")
#     logger.info(f"Attack Method: {opt.method}")

#     # 3. è®¾ç½®éšæœºç§å­ (ä¿ç•™ä½ çš„ä¿®å¤)
#     if hasattr(opt, 'seed'):
#         seed = opt.seed
#     else:
#         seed = 42
    
#     logger.info(f"Setting Random Seed: {seed}")
#     torch.manual_seed(seed)
#     if torch.cuda.is_available():
#         torch.cuda.manual_seed_all(seed)
#     np.random.seed(seed)
#     random.seed(seed)

#     # 4. åŠ è½½æ•°æ® (ä¿ç•™ä½ çš„ä¿®å¤ï¼Œæ‰‹åŠ¨åŠ è½½ JSON)
#     logger.info(f"ğŸ“‚ Loading dataset from: {opt.data_dir}")
#     with open(opt.data_dir, 'r', encoding='utf-8') as f:
#         dataset = json.load(f)

#     # ğŸ‘‡ ä¸´æ—¶åŠ ä¸Šè¿™ä¸€è¡Œï¼Œåªè·‘å‰ 5 ä¸ªï¼Œç”¨æ¥è°ƒè¯•
#     dataset = dataset[:5]
    
#     # ==========================================
#     # ğŸ”§ã€å…³é”®ä¿®å¤ã€‘æ ¼å¼å…¼å®¹æ€§å¤„ç† (Hotfix)
#     # GARAG ä»£ç åªè®¤ "context" å­—æ®µï¼Œä½† BEIR æ•°æ®é›†é€šå¸¸å« "text"
#     # æˆ‘ä»¬åœ¨è¿™é‡Œéå†ä¸€éï¼ŒæŠŠ text çš„å†…å®¹å¤åˆ¶ç»™ context
#     # ==========================================
#     logger.info("ğŸ”§ Pre-processing data: Mapping 'text' to 'context'...")
#     fixed_count = 0
#     for item in dataset:
#         if 'ctxs' in item:
#             for ctx in item['ctxs']:
#                 # å¦‚æœæœ‰ text ä½†æ²¡ contextï¼Œå°±è¡¥ä¸Š context
#                 if 'text' in ctx and 'context' not in ctx:
#                     ctx['context'] = ctx['text']
#                     fixed_count += 1
#     logger.info(f"âœ… Data fixed! Updated {fixed_count} documents.")
#     # ==========================================

#     # 5. æ„å»ºæ”»å‡»å™¨
#     # æ³¨æ„ï¼šbuild_attack è¿”å› (attacker, dataset)
#     attack, dataset = build_attack(opt, dataset)

#     # 6. æ‰§è¡Œæ”»å‡» (æ¢å¤åŸä½œè€…çš„åˆ†æ”¯é€»è¾‘ï¼Œæ›´å¥å£®)
#     logger.info("ğŸš€ Starting Attack...")
#     if opt.is_black:
#         # é»‘ç›’æ”»å‡»é€šå¸¸è°ƒç”¨ augment_dataset
#         result = attack.augment_dataset(dataset)
#     elif opt.is_genetic:
#         # é—ä¼ ç®—æ³•è°ƒç”¨ attack_dataset
#         result = attack.attack_dataset(dataset)
#     elif opt.is_hotflip:
#         # HotFlip ä¹Ÿè°ƒç”¨ attack_dataset
#         result = attack.attack_dataset(dataset)
#     else:
#         # é»˜è®¤æƒ…å†µ
#         result = attack.attack_dataset(dataset)

#     logger.info(f"Attack finished. Generated {len(result)} samples.")

#     # 7. è¯„ä¼°ç»“æœ
#     if len(result) > 0:
#         # å°è¯•æ•è·è¯„ä¼°æ—¶çš„é”™è¯¯ï¼Œé˜²æ­¢æœ€åä¸€æ­¥å´©äº†
#         try:
#             evaluate(result)
#         except Exception as e:
#             logger.error(f"Evaluation failed: {e}")

#     # 8. ä¿å­˜ç»“æœ (ä¿ç•™ä½ çš„åºåˆ—åŒ–ä¿®å¤)
#     if opt.is_save:
#         logger.info("Processing data for saving...")
#         clean_result = make_serializable(result)
        
#         # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
#         os.makedirs(opt.output_dir, exist_ok=True)
        
#         output_file = os.path.join(opt.output_dir, "{}.json".format(opt.method))
#         logger.info(f"ğŸ’¾ Saving results to: {output_file}")
        
#         with open(output_file, 'w', encoding='utf-8') as f:
#             json.dump(clean_result, f, indent=4)

# if __name__ == "__main__":
#     main()


import random
import torch
import numpy as np
from src.option import Options
from src.util import init_logger, timestr
from src.attacker import build_attack
from src.task import evaluate
import logging
import os
import json

# è®¾ç½® logger
logger = logging.getLogger(__name__)

# ç¦ç”¨ Tokenizers å¹¶è¡Œï¼Œé˜²æ­¢æ­»é”
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ==========================================
# ğŸ”§ã€æ ¸æ­¦å™¨çº§ä¿®å¤ã€‘è‡ªå®šä¹‰ JSON ç¼–ç å™¨
# ä¸“é—¨è§£å†³ Tensor/Numpy åµŒå¥—è¿‡æ·±æ— æ³•ä¿å­˜çš„é—®é¢˜
# ==========================================
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, torch.Tensor):
            return obj.item() if obj.numel() == 1 else obj.tolist()
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.float32, np.float64)):
            return float(obj)
        if isinstance(obj, (np.int32, np.int64)):
            return int(obj)
        return super(NumpyEncoder, self).default(obj)
# ==========================================

def main():
    # 1. è§£æå‚æ•°
    t = timestr()
    op = Options("attack")
    opt, message = op.parse(t)

    # 2. åˆå§‹åŒ–æ—¥å¿—
    global logger
    logger = init_logger(opt)
    logger.info(message)
    logger.info(f"Experiment Name: {opt.name}")
    logger.info(f"Attack Method: {opt.method}")

    # 3. è®¾ç½®éšæœºç§å­
    seed = getattr(opt, 'seed', 42)
    logger.info(f"Setting Random Seed: {seed}")
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)

    # 4. åŠ è½½æ•°æ®
    logger.info(f"ğŸ“‚ Loading dataset from: {opt.data_dir}")
    if not os.path.exists(opt.data_dir):
        logger.error(f"âŒ Error: Dataset file not found at {opt.data_dir}")
        return

    with open(opt.data_dir, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
        
    # ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šå¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ GARAG_DEBUG_LIMITï¼Œåˆ™æˆªå–éƒ¨åˆ†æ•°æ®
    debug_limit = os.environ.get("GARAG_DEBUG_LIMIT")
    if debug_limit:
        try:
            limit = int(debug_limit)
            logger.info(f"ğŸ› Debug mode active: Limiting dataset to first {limit} examples.")
            dataset = dataset[:limit]
        except ValueError:
            logger.warning(f"âš ï¸ Invalid GARAG_DEBUG_LIMIT value: {debug_limit}. Ignoring.")

    # 5. æ•°æ®æ ¼å¼çƒ­ä¿®å¤ (text -> context)
    logger.info("ğŸ”§ Pre-processing data: Mapping 'text' to 'context'...")
    for item in dataset:
        if 'ctxs' in item:
            for ctx in item['ctxs']:
                if 'text' in ctx and 'context' not in ctx:
                    ctx['context'] = ctx['text']

    # 6. æ„å»ºæ”»å‡»å™¨
    attack, dataset = build_attack(opt, dataset)

    # 7. æ‰§è¡Œæ”»å‡»
    logger.info("ğŸš€ Starting Attack...")
    if opt.is_black:
        result = attack.augment_dataset(dataset)
    elif opt.is_genetic:
        result = attack.attack_dataset(dataset)
    elif opt.is_hotflip:
        result = attack.attack_dataset(dataset)
    else:
        result = attack.attack_dataset(dataset)

    logger.info(f"Attack finished. Generated {len(result)} samples.")

    # 8. è¯„ä¼°ç»“æœ
    if len(result) > 0:
        try:
            evaluate(result)
        except Exception as e:
            logger.error(f"Evaluation warning: {e}")

    # 9. ä¿å­˜ç»“æœ (ä½¿ç”¨ NumpyEncoder)
    if opt.is_save:
        logger.info("Processing data for saving...")
        
        os.makedirs(opt.output_dir, exist_ok=True)
        output_file = os.path.join(opt.output_dir, "{}.json".format(opt.method))
        logger.info(f"ğŸ’¾ Saving results to: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            # cls=NumpyEncoder æ˜¯å…³é”®ï¼
            json.dump(result, f, indent=4, cls=NumpyEncoder) 
            
    logger.info("âœ… All Done!")

if __name__ == "__main__":
    main()
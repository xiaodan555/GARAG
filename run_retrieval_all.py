import random
import json
import os
import logging
from beir import util, LoggingHandler
from beir.retrieval.evaluation import EvaluateRetrieval
from beir.retrieval.search.dense import DenseRetrievalExactSearch as DenseRetrieval
from beir.datasets.data_loader import GenericDataLoader
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Union
import numpy as np

# === Custom SentenceBERT Wrapper to bypass beir.retrieval.models import issues ===
class SentenceBERT:
    def __init__(self, model_path: str, sep: str = " ", **kwargs):
        self.sep = sep
        self.q_model = SentenceTransformer(model_path)
        self.doc_model = self.q_model
    
    def encode_queries(self, queries: List[str], batch_size: int = 16, **kwargs) -> np.ndarray:
        return self.q_model.encode(queries, batch_size=batch_size, **kwargs)
    
    def encode_corpus(self, corpus: List[Dict[str, str]], batch_size: int = 8, **kwargs) -> np.ndarray:
        sentences = [(doc["title"] + self.sep + doc["text"]).strip() if "title" in doc else doc["text"].strip() for doc in corpus]
        return self.doc_model.encode(sentences, batch_size=batch_size, **kwargs)

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ (åªæ”¹è¿™é‡Œ) =================

# 1. ä½ å½“å‰è¦è·‘çš„æ•°æ®é›†åå­—åˆ—è¡¨
# é€‰é¡¹: "nq", "hotpotqa", "msmarco" (æ³¨æ„æ–‡ä»¶å¤¹åå­—è¦å’Œä½ è§£å‹çš„ä¸€è‡´)
DATASETS = ["nq", "hotpotqa", "msmarco"]

# 2. ä½ çš„ BEIR æ•°æ®æ ¹ç›®å½• (çˆ¶ç›®å½•)
BEIR_ROOT_DIR = "data/beir"

# 3. é‡‡æ ·è®¾ç½®
SAMPLE_SIZE = 100
SEED = 2026

# ================= è‡ªåŠ¨ç”Ÿæˆè·¯å¾„ (ä¸ç”¨æ”¹) =================
MODEL_NAME = "facebook/contriever"
# ========================================================

def process_dataset(dataset_name, retriever):
    data_path = os.path.join(BEIR_ROOT_DIR, dataset_name)
    output_run_file = os.path.join(data_path, f"run_contriever_{dataset_name}_top100.json")
    output_sampled_qids = os.path.join(data_path, f"sampled_{dataset_name}_100_qids.json")

    print(f"\nğŸš€ æ­£åœ¨å¯åŠ¨æ£€ç´¢ä»»åŠ¡: [ {dataset_name} ]")
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_path}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜ä¸º: {output_run_file}\n")

    # ---------------------------------------------------------
    # ç¬¬ä¸€æ­¥ï¼šåŠ è½½ BEIR æ•°æ® (å·²ä¿®æ­£ API)
    # ---------------------------------------------------------
    if not os.path.exists(data_path):
        logging.error(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç›®å½• {data_path}ï¼Œè·³è¿‡è¯¥æ•°æ®é›†ï¼")
        return

    logging.info(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name} ...")
    
    try:
        # === è‡ªåŠ¨åˆ¤æ–­åŠ è½½ test è¿˜æ˜¯ dev ===
        # æœ‰äº›æ•°æ®é›†(å¦‚MSMARCO)å¯èƒ½åªæœ‰ dev.tsvï¼Œæ²¡æœ‰ test.tsv
        # === æ™ºèƒ½åˆ¤æ–­åŠ è½½ split ===
        split_to_load = "test"
        
        # ğŸ”§ ç‰¹æ®Šä¿®æ­£ï¼šMS MARCO å¿…é¡»å¼ºåˆ¶ç”¨ devï¼Œå› ä¸ºå®ƒçš„ test é›†é€šå¸¸æ— æ•ˆæˆ–æ— ç­”æ¡ˆ
        if dataset_name == "msmarco":
            split_to_load = "dev"
            logging.info("ğŸ”§ æ£€æµ‹åˆ° MS MARCOï¼Œå¼ºåˆ¶åˆ‡æ¢ä¸º [dev] é›†æ¨¡å¼")
            
        # å…¶ä»–æ•°æ®é›†å¦‚æœæ‰¾ä¸åˆ° testï¼Œæ‰å›é€€åˆ° dev
        elif not os.path.exists(os.path.join(data_path, "qrels", "test.tsv")):
            if os.path.exists(os.path.join(data_path, "qrels", "dev.tsv")):
                split_to_load = "dev"
            else:
                logging.warning("âš ï¸ æ—¢æ²¡æ‰¾åˆ° test ä¹Ÿæ²¡æ‰¾åˆ° dev qrelsï¼Œå°†å°è¯•åŠ è½½ test (å¯èƒ½ä¼šæŠ¥é”™)...")
        
        # === ä½¿ç”¨ GenericDataLoader åŠ è½½ ===
        corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=split_to_load)
        
    except Exception as e:
        logging.error(f"âŒ åŠ è½½å¤±è´¥ï¼è¯·ç¡®è®¤è¯¥ç›®å½•ä¸‹æœ‰ corpus.jsonl, queries.jsonl å’Œ qrels æ–‡ä»¶å¤¹ã€‚\né”™è¯¯ä¿¡æ¯: {e}")
        return
    
    logging.info(f"  - Corpus (æ–‡æ¡£åº“) å¤§å°: {len(corpus)} æ¡")
    logging.info(f"  - Queries (é—®é¢˜é›†) å¤§å°: {len(queries)} æ¡")

    # ---------------------------------------------------------
    # ç¬¬äºŒæ­¥ï¼šéšæœºæŠ½å– 100 ä¸ªé—®é¢˜
    # ---------------------------------------------------------
    logging.info(f"æ­£åœ¨éšæœºæŠ½å– {SAMPLE_SIZE} ä¸ªé—®é¢˜...")
    random.seed(SEED)
    all_qids = list(queries.keys())
    
    # è¿‡æ»¤ï¼šåªä¿ç•™æœ‰æ ‡å‡†ç­”æ¡ˆçš„é—®é¢˜
    valid_qids = [qid for qid in all_qids if qid in qrels]
    
    if len(valid_qids) < SAMPLE_SIZE:
        logging.warning(f"âš ï¸ è­¦å‘Šï¼šæœ‰æ•ˆé—®é¢˜æ•° ({len(valid_qids)}) å°‘äºé‡‡æ ·æ•°ï¼Œå°†ä½¿ç”¨æ‰€æœ‰é—®é¢˜ã€‚")
        sampled_qids = valid_qids
    else:
        sampled_qids = random.sample(valid_qids, SAMPLE_SIZE)
        
    # æ„å»ºå° Queries å­—å…¸
    small_queries = {qid: queries[qid] for qid in sampled_qids}
    
    # å¤‡ä»½æŠ½æ ·çš„ ID
    with open(output_sampled_qids, 'w') as f:
        json.dump(sampled_qids, f)
    logging.info(f"  - å·²é”å®š {len(small_queries)} ä¸ªæµ‹è¯•é—®é¢˜ (IDå·²å¤‡ä»½)")

    # ---------------------------------------------------------
    # ç¬¬ä¸‰æ­¥ï¼šå…¨åº“æ£€ç´¢ (æ¨¡å‹å·²åœ¨å¤–éƒ¨åŠ è½½)
    # ---------------------------------------------------------
    logging.info("ğŸ”¥ å¼€å§‹å…¨åº“æ£€ç´¢ (Indexing Corpus)...")
    
    results = retriever.retrieve(corpus, small_queries)

    # ---------------------------------------------------------
    # ç¬¬å››æ­¥ï¼šä¿å­˜ Top-100 ç»“æœ
    # ---------------------------------------------------------
    logging.info(f"æ­£åœ¨ä¿å­˜æ£€ç´¢ç»“æœåˆ°: {output_run_file}")
    
    top_k_results = {}
    for qid, docs in results.items():
        # æ’åºå¹¶æˆªå– Top-100
        sorted_docs = sorted(docs.items(), key=lambda item: item[1], reverse=True)[:100]
        top_k_results[qid] = {k: v for k, v in sorted_docs}

    with open(output_run_file, 'w') as f:
        json.dump(top_k_results, f, indent=4)
        
    logging.info(f"âœ… [ {dataset_name} ] ä»»åŠ¡å®Œæˆï¼")
    logging.info(f"ç»“æœå·²ç”Ÿæˆ: {output_run_file}")

def main():
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(format='%(asctime)s - %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        level=logging.INFO,
                        handlers=[LoggingHandler()])
    
    # ---------------------------------------------------------
    # åŠ è½½ Contriever æ¨¡å‹ (åªåŠ è½½ä¸€æ¬¡)
    # ---------------------------------------------------------
    logging.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME} ...")
    model = DenseRetrieval(SentenceBERT(MODEL_NAME), batch_size=128)
    retriever = EvaluateRetrieval(model, score_function="dot")

    # ---------------------------------------------------------
    # å¾ªç¯å¤„ç†æ¯ä¸ªæ•°æ®é›†
    # ---------------------------------------------------------
    for dataset_name in DATASETS:
        process_dataset(dataset_name, retriever)
        
    logging.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    main()
import random
import json
import os
import logging
from beir import util, LoggingHandler
from beir.retrieval import models
from beir.retrieval.evaluation import EvaluateRetrieval
from beir.retrieval.search.dense import DenseRetrievalExactSearch as DenseRetrieval
# === å…³é”®ä¿®æ­£ï¼šå¼•å…¥æ­£ç¡®çš„æ•°æ®åŠ è½½å™¨ ===
from beir.datasets.data_loader import GenericDataLoader

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ (åªæ”¹è¿™é‡Œ) =================

# 1. ä½ å½“å‰è¦è·‘çš„æ•°æ®é›†åå­—
# é€‰é¡¹: "nq", "hotpotqa", "msmarco" (æ³¨æ„æ–‡ä»¶å¤¹åå­—è¦å’Œä½ è§£å‹çš„ä¸€è‡´)
DATASET_NAME = "msmarco"  

# 2. ä½ çš„ BEIR æ•°æ®æ ¹ç›®å½• (çˆ¶ç›®å½•)
BEIR_ROOT_DIR = "data/beir"

# 3. é‡‡æ ·è®¾ç½®
SAMPLE_SIZE = 100
SEED = 2026

# ================= è‡ªåŠ¨ç”Ÿæˆè·¯å¾„ (ä¸ç”¨æ”¹) =================
DATA_PATH = os.path.join(BEIR_ROOT_DIR, DATASET_NAME)
OUTPUT_RUN_FILE = os.path.join(DATA_PATH, f"run_contriever_{DATASET_NAME}_top100.json")
OUTPUT_SAMPLED_QIDS = os.path.join(DATA_PATH, f"sampled_{DATASET_NAME}_100_qids.json")
MODEL_NAME = "facebook/contriever"
# ========================================================

def main():
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(format='%(asctime)s - %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        level=logging.INFO,
                        handlers=[LoggingHandler()])
    
    print(f"\nğŸš€ æ­£åœ¨å¯åŠ¨æ£€ç´¢ä»»åŠ¡: [ {DATASET_NAME} ]")
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {DATA_PATH}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜ä¸º: {OUTPUT_RUN_FILE}\n")

    # ---------------------------------------------------------
    # ç¬¬ä¸€æ­¥ï¼šåŠ è½½ BEIR æ•°æ® (å·²ä¿®æ­£ API)
    # ---------------------------------------------------------
    if not os.path.exists(DATA_PATH):
        logging.error(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç›®å½• {DATA_PATH}ï¼Œè¯·æ£€æŸ¥ DATASET_NAME æ˜¯å¦å†™å¯¹ï¼")
        return

    logging.info(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {DATASET_NAME} ...")
    
    try:
        # === è‡ªåŠ¨åˆ¤æ–­åŠ è½½ test è¿˜æ˜¯ dev ===
        # æœ‰äº›æ•°æ®é›†(å¦‚MSMARCO)å¯èƒ½åªæœ‰ dev.tsvï¼Œæ²¡æœ‰ test.tsv
        # === æ™ºèƒ½åˆ¤æ–­åŠ è½½ split ===
        split_to_load = "test"
        
        # ğŸ”§ ç‰¹æ®Šä¿®æ­£ï¼šMS MARCO å¿…é¡»å¼ºåˆ¶ç”¨ devï¼Œå› ä¸ºå®ƒçš„ test é›†é€šå¸¸æ— æ•ˆæˆ–æ— ç­”æ¡ˆ
        if DATASET_NAME == "msmarco":
            split_to_load = "dev"
            logging.info("ğŸ”§ æ£€æµ‹åˆ° MS MARCOï¼Œå¼ºåˆ¶åˆ‡æ¢ä¸º [dev] é›†æ¨¡å¼")
            
        # å…¶ä»–æ•°æ®é›†å¦‚æœæ‰¾ä¸åˆ° testï¼Œæ‰å›é€€åˆ° dev
        elif not os.path.exists(os.path.join(DATA_PATH, "qrels", "test.tsv")):
            if os.path.exists(os.path.join(DATA_PATH, "qrels", "dev.tsv")):
                split_to_load = "dev"
            else:
                logging.warning("âš ï¸ æ—¢æ²¡æ‰¾åˆ° test ä¹Ÿæ²¡æ‰¾åˆ° dev qrelsï¼Œå°†å°è¯•åŠ è½½ test (å¯èƒ½ä¼šæŠ¥é”™)...")
        
        # === ä½¿ç”¨ GenericDataLoader åŠ è½½ ===
        corpus, queries, qrels = GenericDataLoader(data_folder=DATA_PATH).load(split=split_to_load)
        
    except Exception as e:
        logging.error(f"âŒ åŠ è½½å¤±è´¥ï¼è¯·ç¡®è®¤è¯¥ç›®å½•ä¸‹æœ‰ corpus.jsonl, queries.jsonl å’Œ qrels æ–‡ä»¶å¤¹ã€‚\né”™è¯¯ä¿¡æ¯: {e}")
        return
    
    logging.info(f"  - Corpus (æ–‡æ¡£åº“) å¤§å°: {len(corpus)} æ¡")
    logging.info(f"  - Queries (é—®é¢˜é›†) å¤§å°: {len(queries)} æ¡")

    # ---------------------------------------------------------
    # ç¬¬äºŒæ­¥ï¼šéšæœºæŠ½å– 100 ä¸ªé—®é¢˜
    # ---------------------------------------------------------
    logging.info(f"æ­£åœ¨éšæœºæŠ½å– {SAMPLE_SIZE} ä¸ªé—®é¢˜...")
    random.seed(SEED)
    all_qids = list(queries.keys())
    
    # è¿‡æ»¤ï¼šåªä¿ç•™æœ‰æ ‡å‡†ç­”æ¡ˆçš„é—®é¢˜
    valid_qids = [qid for qid in all_qids if qid in qrels]
    
    if len(valid_qids) < SAMPLE_SIZE:
        logging.warning(f"âš ï¸ è­¦å‘Šï¼šæœ‰æ•ˆé—®é¢˜æ•° ({len(valid_qids)}) å°‘äºé‡‡æ ·æ•°ï¼Œå°†ä½¿ç”¨æ‰€æœ‰é—®é¢˜ã€‚")
        sampled_qids = valid_qids
    else:
        sampled_qids = random.sample(valid_qids, SAMPLE_SIZE)
        
    # æ„å»ºå° Queries å­—å…¸
    small_queries = {qid: queries[qid] for qid in sampled_qids}
    
    # å¤‡ä»½æŠ½æ ·çš„ ID
    with open(OUTPUT_SAMPLED_QIDS, 'w') as f:
        json.dump(sampled_qids, f)
    logging.info(f"  - å·²é”å®š {len(small_queries)} ä¸ªæµ‹è¯•é—®é¢˜ (IDå·²å¤‡ä»½)")

    # ---------------------------------------------------------
    # ç¬¬ä¸‰æ­¥ï¼šåŠ è½½ Contriever æ¨¡å‹
    # ---------------------------------------------------------
    logging.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME} ...")
    model = DenseRetrieval(models.SentenceBERT(MODEL_NAME), batch_size=128)
    retriever = EvaluateRetrieval(model, score_function="dot")

    # ---------------------------------------------------------
    # ç¬¬å››æ­¥ï¼šå…¨åº“æ£€ç´¢
    # ---------------------------------------------------------
    logging.info("ğŸ”¥ å¼€å§‹å…¨åº“æ£€ç´¢ (Indexing Corpus)...")
    
    results = retriever.retrieve(corpus, small_queries)

    # ---------------------------------------------------------
    # ç¬¬äº”æ­¥ï¼šä¿å­˜ Top-100 ç»“æœ
    # ---------------------------------------------------------
    logging.info(f"æ­£åœ¨ä¿å­˜æ£€ç´¢ç»“æœåˆ°: {OUTPUT_RUN_FILE}")
    
    top_k_results = {}
    for qid, docs in results.items():
        # æ’åºå¹¶æˆªå– Top-100
        sorted_docs = sorted(docs.items(), key=lambda item: item[1], reverse=True)[:100]
        top_k_results[qid] = {k: v for k, v in sorted_docs}

    with open(OUTPUT_RUN_FILE, 'w') as f:
        json.dump(top_k_results, f, indent=4)
        
    logging.info(f"âœ… [ {DATASET_NAME} ] ä»»åŠ¡å®Œæˆï¼")
    logging.info(f"ç»“æœå·²ç”Ÿæˆ: {OUTPUT_RUN_FILE}")

if __name__ == "__main__":
    main()